{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  4. Ejemplo End-to-End Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación Oficial Structured Spark Streaming**: http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**: https://blog.knoldus.com/basic-example-spark-structured-streaming-kafka-integration/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Instrucciones iniciales y setup\n",
    "\n",
    "2. Crea un directorio `checkpoint` dentro del subdirectorio `data`.\n",
    "\n",
    "3. Asegúrate de que tienes permisos suficientes para manipular archivos dentro del directorio (debería ser así ya, si has ejecutado los ejemplos previos). Si fuese necesario, ejecuta `sudo chmod -R 777 data`.\n",
    "\n",
    "**Entrada: cola de Kafka**\n",
    "\n",
    "4. Arranca el broker de Kafka, o bien localmente instalado o en una MV local o en un contenedor local (e.g. Docker).\n",
    "\n",
    "5. Modifica el script de Python `4-kafka_producer.py` para que envíe los datos al broker de Kafka (indicar la IP y puerto correctos).\n",
    "\n",
    "6. Activa si es necesario el entorno de Anaconda Python (**importante, usando Python v3.6+**). Ejecuta el productor de Kafka con `python p_kafka_producer.py 0.6 1.3 test data/occupancy_data.csv`.\n",
    "\n",
    "7. A partir de ese momento ya estás listo para ejecutar los *jobs* de Spark Streaming de este notebook. ¡Empecemos con el análisis!\n",
    "\n",
    "**WebUI**: Mientras el contexto de Spark Streaming esté activo, podemos acceder a la interfaz de monitorización de los *jobs* en http://localhost:4040."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Importaciones y creación del contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creación del SparkContext (solo la primera vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de dependencias y funciones\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from operator import add\n",
    "from operator import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_SUBMIT_ARGS =  --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.5 pyspark-shell \n",
      "\n",
      "JAVA_HOME =  /usr/lib/jvm/java-8-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "# Load external packages programatically\n",
    "import os\n",
    "# THIS IS MANDATORY\n",
    "# You must provide the information about the Maven artifact for the\n",
    "# Spark Streaming connector to Kafka\n",
    "# At present time, only the 0.8.2 version (deprecated) has\n",
    "# Python support\n",
    "packages = \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.5\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages {0} pyspark-shell\".format(packages)\n",
    ")\n",
    "# THIS IS COMPULSORY\n",
    "# Comment the line below if JAVA_HOME is already set up or you\n",
    "# only have a single JVM version in your system\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "# OPTIONAL: Check setup of environment variables\n",
    "print(\"PYSPARK_SUBMIT_ARGS = \",os.environ[\"PYSPARK_SUBMIT_ARGS\"],\"\\n\")\n",
    "print(\"JAVA_HOME = \", os.environ[\"JAVA_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"KafkaStreamingEndtoEnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"prueba\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del streaming context (en cada ejecución de ejercicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el contexto de Spark Streaming\n",
    "# Intervalo de actualización de micro-batches (triggers): 5s\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Métodos auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Método de parseo de datos meteorológicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método nos ayuda a parsear cada línea que llega por la cola de Kafka con datos meteorológicos. Lo utilizamos para acceder a los datos de cada evento (orden) del *stream* de entrada de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parseOrder(line):\n",
    "  s = line.split(\",\")\n",
    "  try:\n",
    "      return [{\"id\": s[0],\n",
    "               \"date\": s[1],\n",
    "               \"Temperature\": s[2], \n",
    "               \"Humidity\": s[3],\n",
    "               \"Light\": s[4],\n",
    "               \"CO2\": s[5],\n",
    "               \"HumidityRatio\": s[6],\n",
    "               \"Occupancy\": s[7]}]\n",
    "\n",
    "  except Exception as err:\n",
    "      print(\"Wrong line format (%s): \" % line)\n",
    "      return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Métodos de escritura - Envío de datos a Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método contiene un *productor singleton* (para evitar tener más de un productor enviando datos al broker de Kafka) y un método para serializar los resultados en formato CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configura el endpoint para localizar el broker de Kafka\n",
    "# kafkaBrokerIPPort = \"172.20.1.21:9092\"\n",
    "kafkaBrokerIPPort = \"127.0.0.1:9092\"\n",
    "\n",
    "# Productor simple (Singleton!)\n",
    "# from kafka import KafkaProducer\n",
    "import kafka\n",
    "class KafkaProducerWrapper(object):\n",
    "  producer = None\n",
    "  @staticmethod\n",
    "  def getProducer(brokerList):\n",
    "    if KafkaProducerWrapper.producer != None:\n",
    "      return KafkaProducerWrapper.producer\n",
    "    else:\n",
    "      KafkaProducerWrapper.producer = kafka.KafkaProducer(bootstrap_servers=brokerList,\n",
    "                                                          key_serializer=str.encode,\n",
    "                                                          value_serializer=str.encode)\n",
    "      return KafkaProducerWrapper.producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Fuente de datos - Lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "Schema = StructType([\n",
    " StructField(\"id\", IntegerType()),\n",
    " StructField(\"date\", StringType()),\n",
    " StructField(\"Temperature\", DoubleType()),\n",
    " StructField(\"Humidity\", DoubleType()),\n",
    " StructField(\"Light\", DoubleType()),\n",
    " StructField(\"CO2\", DoubleType()),\n",
    " StructField(\"HumidityRatio\", DoubleType()),\n",
    " StructField(\"Occupancy\", DoubleType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fichero de texto: Lectura de fuente de datos de fichero (no se usa en este ejemplo, en su lugar \n",
    "# enviamos los datos a Kafka para crear una simulación más realista)\n",
    "stream = ssc.textFileStream(\"data/occupancy_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.schema(Schema).csv(\"data/occupancy_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .schema(Schema) \\\n",
    "    .csv(\"data/occupancy_data.csv\")  # Equivalent to format(\"csv\").load(\"/path/to/directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e07ee9255390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misStreaming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Returns True for DataFrames that have streaming sources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "df.isStreaming()    # Returns True for DataFrames that have streaming sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      " |-- Light: double (nullable = true)\n",
      " |-- CO2: double (nullable = true)\n",
      " |-- HumidityRatio: double (nullable = true)\n",
      " |-- Occupancy: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Temperature: double]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"Temperature\").where(\"Temperature > 15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"id\",\"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrada de datos desde Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka: Lectura de datos\n",
    "kafkaParams = {\"metadata.broker.list\": kafkaBrokerIPPort}\n",
    "stream = KafkaUtils.createDirectStream(ssc, [\"test\"], kafkaParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream = stream.map(lambda o: str(o[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Calcular el promedio de valores de Temperatura, humedad relativa y concentración de CO2 para cada micro-batch y el promedio de dichos valores desde el arranque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prueba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_score(col):\n",
    "    return pd.Series([np.mean(col)] * len(col))\n",
    "\n",
    "def process():\n",
    "    \n",
    "    data = spark.read.json(rdd) #, schema = schema\n",
    "\n",
    "    data = data.withColumn('mean_score', mean_score(data['score']))\n",
    "\n",
    "    data.show()\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream.foreachRDD(lambda rdd: rdd.foreachPartition(process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = stream.flatMap(parseOrder)\n",
    "\n",
    "contar_temp = valores.map(lambda o: (o['Temperature'], 1)).reduceByKey(add)\n",
    "\n",
    "#amountPerClient = valores.map(lambda o: (o['clientId'], o['amount'] * o['price']))\n",
    "\n",
    "conteo_temp = (contar_temp.updateStateByKey(lambda vals, \n",
    "                                            total_count: sum(vals) + total_count if total_count != None else sum(vals)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Queries with streaming sources must be executed with writeStream.start();;\\nsocket'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o65.showString.\n: org.apache.spark.sql.AnalysisException: Queries with streaming sources must be executed with writeStream.start();;\nsocket\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:389)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:36)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:36)\n\tat org.apache.spark.sql.execution.QueryExecution.assertSupported(QueryExecution.scala:52)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:61)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:69)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:78)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3365)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e226db09659a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Queries with streaming sources must be executed with writeStream.start();;\\nsocket'"
     ]
    }
   ],
   "source": [
    "lines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temp_mean = valores.map(lambda o: (o['Temperature'], o)).reduceByKey(mean_score)\n",
    "#Temp_mean = (valores.updateStateByKey(lambda vals, \n",
    "#                                      totalOpt: sum(vals) + totalOpt if totalOpt != None else sum(vals)))\n",
    "\n",
    "\n",
    "\n",
    "#Temp_mean.repartition(1).saveAsTextFiles(\"data/output/metrics\", \"csv\")\n",
    "\n",
    "#lines = valores.map(lambda x: x[1])\n",
    "#counts = lines.map(lambda line: line.split(\"\\t\")) \\\n",
    "#              .reduceByKey(lambda a, b: a+b)\n",
    "\n",
    "#Temp_mean.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o26.start.\n: java.lang.IllegalArgumentException: requirement failed: The checkpoint directory has not been set. Please set it by StreamingContext.checkpoint().\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:243)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:276)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.streaming.DStreamGraph.start(DStreamGraph.scala:51)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.startFirstTime(JobGenerator.scala:194)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.start(JobGenerator.scala:100)\n\tat org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:103)\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:588)\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:583)\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:583)\n\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n\tat org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:583)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:575)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:556)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9eb6e7f74a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mStart\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \"\"\"\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mStreamingContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activeContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.start.\n: java.lang.IllegalArgumentException: requirement failed: The checkpoint directory has not been set. Please set it by StreamingContext.checkpoint().\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:243)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)\n\tat org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:276)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.streaming.DStreamGraph.start(DStreamGraph.scala:51)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.startFirstTime(JobGenerator.scala:194)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.start(JobGenerator.scala:100)\n\tat org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:103)\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:588)\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:583)\n\tat org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:583)\n\tat ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()\n\tat org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:583)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:575)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:556)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you are done, stop the StreamingContext\n",
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tags_count(new_values, total_sum):\n",
    "        return sum(new_values) + (total_sum or 0)\n",
    "    \n",
    "# divide cada Tweet en palabras\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "# filtra las palabras para obtener solo hashtags, luego mapea cada hashtag para que sea un par de (hashtag,1)\n",
    "hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))\n",
    "# agrega la cuenta de cada hashtag a su última cuenta\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:24:35\n",
      "-------------------------------------------\n",
      "{'id': '\"1781\"', 'date': '\"2015-02-05 23:31:00\"', 'Temperature': '20.39', 'Humidity': '21.29', 'Light': '0', 'CO2': '444.5', 'HumidityRatio': '0.00314694763447223', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1782\"', 'date': '\"2015-02-05 23:31:59\"', 'Temperature': '20.3566666666667', 'Humidity': '21.29', 'Light': '0', 'CO2': '441.666666666667', 'HumidityRatio': '0.00314044248260255', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1783\"', 'date': '\"2015-02-05 23:32:59\"', 'Temperature': '20.3566666666667', 'Humidity': '21.29', 'Light': '0', 'CO2': '442.333333333333', 'HumidityRatio': '0.00314044248260255', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1784\"', 'date': '\"2015-02-05 23:34:00\"', 'Temperature': '20.39', 'Humidity': '21.29', 'Light': '0', 'CO2': '441', 'HumidityRatio': '0.00314694763447223', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1785\"', 'date': '\"2015-02-05 23:35:00\"', 'Temperature': '20.29', 'Humidity': '21.29', 'Light': '0', 'CO2': '440.5', 'HumidityRatio': '0.00312746769855435', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1786\"', 'date': '\"2015-02-05 23:36:00\"', 'Temperature': '20.39', 'Humidity': '21.29', 'Light': '0', 'CO2': '447', 'HumidityRatio': '0.00314694763447223', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1787\"', 'date': '\"2015-02-05 23:37:00\"', 'Temperature': '20.34', 'Humidity': '21.29', 'Light': '0', 'CO2': '439.5', 'HumidityRatio': '0.00313719435010793', 'Occupancy': '0\\n'}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:24:40\n",
      "-------------------------------------------\n",
      "{'id': '\"1788\"', 'date': '\"2015-02-05 23:38:00\"', 'Temperature': '20.29', 'Humidity': '21.29', 'Light': '0', 'CO2': '444', 'HumidityRatio': '0.00312746769855435', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1789\"', 'date': '\"2015-02-05 23:38:59\"', 'Temperature': '20.29', 'Humidity': '21.29', 'Light': '0', 'CO2': '445.5', 'HumidityRatio': '0.00312746769855435', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1790\"', 'date': '\"2015-02-05 23:39:59\"', 'Temperature': '20.29', 'Humidity': '21.29', 'Light': '0', 'CO2': '446.666666666667', 'HumidityRatio': '0.00312746769855435', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1791\"', 'date': '\"2015-02-05 23:41:00\"', 'Temperature': '20.3233333333333', 'Humidity': '21.29', 'Light': '0', 'CO2': '444.333333333333', 'HumidityRatio': '0.00313394917681341', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1792\"', 'date': '\"2015-02-05 23:42:00\"', 'Temperature': '20.29', 'Humidity': '21.29', 'Light': '0', 'CO2': '446.5', 'HumidityRatio': '0.00312746769855435', 'Occupancy': '0\\n'}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:24:45\n",
      "-------------------------------------------\n",
      "{'id': '\"1793\"', 'date': '\"2015-02-05 23:43:00\"', 'Temperature': '20.3233333333333', 'Humidity': '21.29', 'Light': '0', 'CO2': '449.333333333333', 'HumidityRatio': '0.00313394917681341', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1794\"', 'date': '\"2015-02-05 23:44:00\"', 'Temperature': '20.29', 'Humidity': '21.29', 'Light': '0', 'CO2': '445', 'HumidityRatio': '0.00312746769855435', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1795\"', 'date': '\"2015-02-05 23:44:59\"', 'Temperature': '20.29', 'Humidity': '21.29', 'Light': '0', 'CO2': '447', 'HumidityRatio': '0.00312746769855435', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1796\"', 'date': '\"2015-02-05 23:45:59\"', 'Temperature': '20.29', 'Humidity': '21.29', 'Light': '0', 'CO2': '443.5', 'HumidityRatio': '0.00312746769855435', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1797\"', 'date': '\"2015-02-05 23:47:00\"', 'Temperature': '20.29', 'Humidity': '21.29', 'Light': '0', 'CO2': '447', 'HumidityRatio': '0.00312746769855435', 'Occupancy': '0\\n'}\n",
      "{'id': '\"1798\"', 'date': '\"2015-02-05 23:48:00\"', 'Temperature': '20.29', 'Humidity': '21.29', 'Light': '0', 'CO2': '445', 'HumidityRatio': '0.00312746769855435', 'Occupancy': '0\\n'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:00\n",
      "-------------------------------------------\n",
      "\"1621\",\"2015-02-05 20:51:00\",21,19.7,0,463.5,0.00302284822358619,0\n",
      "\n",
      "\"1622\",\"2015-02-05 20:51:59\",21,19.7,0,467.5,0.00302284822358619,0\n",
      "\n",
      "\"1623\",\"2015-02-05 20:53:00\",21,19.7,0,476,0.00302284822358619,0\n",
      "\n",
      "\"1624\",\"2015-02-05 20:54:00\",21,19.76,0,474,0.00303209974808914,0\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:05\n",
      "-------------------------------------------\n",
      "\"1625\",\"2015-02-05 20:55:00\",21,19.79,0,471,0.00303672561304647,0\n",
      "\n",
      "\"1626\",\"2015-02-05 20:55:59\",21,19.79,0,472,0.00303672561304647,0\n",
      "\n",
      "\"1627\",\"2015-02-05 20:57:00\",21,19.79,0,472,0.00303672561304647,0\n",
      "\n",
      "\"1628\",\"2015-02-05 20:57:59\",21,19.79,0,474.25,0.00303672561304647,0\n",
      "\n",
      "\"1629\",\"2015-02-05 20:58:59\",20.9725,19.79,0,473,0.00303157119548328,0\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:10\n",
      "-------------------------------------------\n",
      "\"1630\",\"2015-02-05 21:00:00\",21,19.79,0,469,0.00303672561304647,0\n",
      "\n",
      "\"1631\",\"2015-02-05 21:01:00\",20.9633333333333,19.79,0,472.666666666667,0.00302985476836808,0\n",
      "\n",
      "\"1632\",\"2015-02-05 21:02:00\",21,19.79,0,473,0.00303672561304647,0\n",
      "\n",
      "\"1633\",\"2015-02-05 21:03:00\",20.89,19.79,0,471.5,0.00301615411603875,0\n",
      "\n",
      "\"1634\",\"2015-02-05 21:04:00\",20.9633333333333,19.79,0,471.333333333333,0.00302985476836808,0\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:15\n",
      "-------------------------------------------\n",
      "\"1635\",\"2015-02-05 21:04:59\",21,19.79,0,469,0.00303672561304647,0\n",
      "\n",
      "\"1636\",\"2015-02-05 21:06:00\",20.89,19.89,0,471,0.00303146919477111,0\n",
      "\n",
      "\"1637\",\"2015-02-05 21:07:00\",20.89,19.84,0,470,0.00302381156158785,0\n",
      "\n",
      "\"1638\",\"2015-02-05 21:08:00\",20.89,19.89,0,469.5,0.00303146919477111,0\n",
      "\n",
      "\"1639\",\"2015-02-05 21:08:59\",20.89,19.89,0,464,0.00303146919477111,0\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:20\n",
      "-------------------------------------------\n",
      "\"1640\",\"2015-02-05 21:10:00\",20.89,19.89,0,467,0.00303146919477111,0\n",
      "\n",
      "\"1641\",\"2015-02-05 21:10:59\",20.89,19.9266666666667,0,472,0.00303708491169476,0\n",
      "\n",
      "\"1642\",\"2015-02-05 21:11:59\",20.89,19.945,0,469.5,0.00303989280799839,0\n",
      "\n",
      "\"1643\",\"2015-02-05 21:13:00\",20.89,20,0,470.5,0.00304831664828052,0\n",
      "\n",
      "\"1644\",\"2015-02-05 21:14:00\",20.89,20,0,470,0.00304831664828052,0\n",
      "\n",
      "\"1645\",\"2015-02-05 21:15:00\",20.89,20,0,469,0.00304831664828052,0\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:25\n",
      "-------------------------------------------\n",
      "\"1646\",\"2015-02-05 21:16:00\",20.89,20,0,468.5,0.00304831664828052,0\n",
      "\n",
      "\"1647\",\"2015-02-05 21:16:59\",20.89,20,0,467.333333333333,0.00304831664828052,0\n",
      "\n",
      "\"1648\",\"2015-02-05 21:17:59\",20.89,20,0,467,0.00304831664828052,0\n",
      "\n",
      "\"1649\",\"2015-02-05 21:19:00\",20.89,20,0,464,0.00304831664828052,0\n",
      "\n",
      "\"1650\",\"2015-02-05 21:20:00\",20.89,20,0,462,0.00304831664828052,0\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:30\n",
      "-------------------------------------------\n",
      "\"1651\",\"2015-02-05 21:21:00\",20.89,20,0,466,0.00304831664828052,0\n",
      "\n",
      "\"1652\",\"2015-02-05 21:22:00\",20.89,20,0,465.5,0.00304831664828052,0\n",
      "\n",
      "\"1653\",\"2015-02-05 21:23:00\",20.79,20,0,464,0.0030295225859519,0\n",
      "\n",
      "\"1654\",\"2015-02-05 21:23:59\",20.89,20,0,469.666666666667,0.00304831664828052,0\n",
      "\n",
      "\"1655\",\"2015-02-05 21:24:59\",20.89,20.1,0,469.5,0.00306363330326149,0\n",
      "\n",
      "\"1656\",\"2015-02-05 21:26:00\",20.89,20.1,0,472,0.00306363330326149,0\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:35\n",
      "-------------------------------------------\n",
      "\"1657\",\"2015-02-05 21:27:00\",20.89,20.1,0,464.5,0.00306363330326149,0\n",
      "\n",
      "\"1658\",\"2015-02-05 21:28:00\",20.89,20.1,0,467,0.00306363330326149,0\n",
      "\n",
      "\"1659\",\"2015-02-05 21:29:00\",20.89,20.1,0,467,0.00306363330326149,0\n",
      "\n",
      "\"1660\",\"2015-02-05 21:29:59\",20.89,20.1,0,462,0.00306363330326149,0\n",
      "\n",
      "\"1661\",\"2015-02-05 21:30:59\",20.89,20.15,0,464.5,0.00307129191225357,0\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:40\n",
      "-------------------------------------------\n",
      "\"1662\",\"2015-02-05 21:32:00\",20.79,20.05,0,460,0.00303713337418589,0\n",
      "\n",
      "\"1663\",\"2015-02-05 21:33:00\",20.89,20.2,0,457,0.00307895070892257,0\n",
      "\n",
      "\"1664\",\"2015-02-05 21:34:00\",20.79,20.1,0,461,0.00304474434777049,0\n",
      "\n",
      "\"1665\",\"2015-02-05 21:35:00\",20.79,20.1,0,462,0.00304474434777049,0\n",
      "\n",
      "\"1666\",\"2015-02-05 21:36:00\",20.79,20.15,0,463.5,0.00305235550671248,0\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:45\n",
      "-------------------------------------------\n",
      "\"1667\",\"2015-02-05 21:36:59\",20.79,20.2,0,461.5,0.00305996685101862,0\n",
      "\n",
      "\"1668\",\"2015-02-05 21:38:00\",20.79,20.2,0,461.5,0.00305996685101862,0\n",
      "\n",
      "\"1669\",\"2015-02-05 21:39:00\",20.79,20.2,0,458,0.00305996685101862,0\n",
      "\n",
      "\"1670\",\"2015-02-05 21:40:00\",20.79,20.2,0,460,0.00305996685101862,0\n",
      "\n",
      "\"1671\",\"2015-02-05 21:40:59\",20.79,20.245,0,460,0.00306681721938609,0\n",
      "\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-06-24 22:22:50\n",
      "-------------------------------------------\n",
      "\"1672\",\"2015-02-05 21:42:00\",20.79,20.29,0,460.5,0.00307366773790896,0\n",
      "\n",
      "\"1673\",\"2015-02-05 21:42:59\",20.745,20.29,0,460,0.0030651203870119,0\n",
      "\n",
      "\"1674\",\"2015-02-05 21:43:59\",20.79,20.29,0,457,0.00307366773790896,0\n",
      "\n",
      "\"1675\",\"2015-02-05 21:45:00\",20.745,20.34,0,460.5,0.00307271097901383,0\n",
      "\n",
      "\"1676\",\"2015-02-05 21:46:00\",20.7,20.39,0,461,0.00307173289826042,0\n",
      "\n",
      "\"1677\",\"2015-02-05 21:47:00\",20.745,20.39,0,458.5,0.00308030175537347,0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Calcular el promedio de luminosidad en la estancia en ventanas deslizantes de tamaño 45 segundos, con un valor de deslizamiento de 15 segundos entre ventanas consecutivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Examinando los datos, podemos apreciar que el intervalo entre muestras originales no es exactamente de 1 minuto en muchos casos. Calcular el número de parejas de muestras consecutivas en cada micro-batch entre las cuales el intervalo de separación no es exactamente de 1 minuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "#kafka.errors.UnrecognizedBrokerVersion: UnrecognizedBrokerVersion\n",
    "#ssc.awaitTerminationOrTimeout(10)  # Espera 10 segs. antes de acabar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parseOrder(line):\n",
    "  s = line.split(\",\")\n",
    "  try:\n",
    "      return [{\"id\": s[0],\n",
    "               \"date\": datetime.strptime(s[1], \"%Y-%m-%d %H:%M:%S\"),\n",
    "               \"Temperature\": float(s[2]), \n",
    "               \"Humidity\": float(s[3]),\n",
    "               \"Light\": s[4],\n",
    "               \"CO2\": float(s[5]),\n",
    "               \"HumidityRatio\": float(s[6]),\n",
    "               \"Occupancy\": s[7]}]\n",
    "\n",
    "  except Exception as err:\n",
    "      print(\"Wrong line format (%s): \" % line)\n",
    "      return []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
