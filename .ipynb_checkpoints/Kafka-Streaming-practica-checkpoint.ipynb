{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Práctica SDPD T3 – 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Enrique Macip Belmonte\n",
    "    Maria Ruiz Teixidor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones iniciales y setup\n",
    "\n",
    "Antes de ejecutar este Notebook, se siguen los siguientes pasos:\n",
    "\n",
    "1. Arranque de Zookeeper\n",
    "2. Arranque de Kafka\n",
    "3. Ejecutamos el archivo p_kafka_producer.py con la siguiente instrucción: `python p_kafka_producer.py 0.6 1.3 test data/occupancy_data.csv`. Con éste estamos indicando que se envíen los datos a la cola test de Kafka con un retardo variable entre muestras insertadas de entre 0.6 y 1.3 segundos.\n",
    "4. Ahora ya se pueden ejecutar los jobs de Spark Streaming del Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Importaciones y creación del contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from operator import add\n",
    "from operator import sub\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_SUBMIT_ARGS =  --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 pyspark-shell \n",
      "\n",
      "JAVA_HOME =  /usr/lib/jvm/java-8-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "# Load external packages programatically\n",
    "import os\n",
    "# THIS IS MANDATORY\n",
    "# You must provide the information about the Maven artifact for the\n",
    "# Spark Streaming connector to Kafka\n",
    "# At present time, only the 0.8.2 version (deprecated) has\n",
    "# Python support\n",
    "#packages = \"org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5\"\n",
    "packages = \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages {0} pyspark-shell\".format(packages)\n",
    ")\n",
    "# THIS IS COMPULSORY\n",
    "# Comment the line below if JAVA_HOME is already set up or you\n",
    "# only have a single JVM version in your system\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "# OPTIONAL: Check setup of environment variables\n",
    "print(\"PYSPARK_SUBMIT_ARGS = \",os.environ[\"PYSPARK_SUBMIT_ARGS\"],\"\\n\")\n",
    "print(\"JAVA_HOME = \", os.environ[\"JAVA_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"prueba\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.159:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>prueba</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2814786bd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Fuente de datos - Lectura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los datos con la API de Spark Structured Streaming, indicand la cola de Kafka 'test':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", 'localhost:9092')\\\n",
    "    .option('subscribe', 'test')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos la estructura de los datos que vienen por Kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario obtener los diferentes campos que contiene 'value', son los que nos interesan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr('CAST(value AS STRING)')\n",
    "\n",
    "df_data = df.select(\n",
    "        split(df.value, ',')[0].alias(\"row\").cast(StringType()),\n",
    "        split(df.value, ',')[1].alias(\"date\").cast(StringType()),\n",
    "        split(df.value, ',')[2].alias(\"Temperature\").cast(DoubleType()),\n",
    "        split(df.value, ',')[3].alias(\"Humidity\").cast(DoubleType()),\n",
    "        split(df.value, ',')[4].alias(\"Light\").cast(DoubleType()),\n",
    "        split(df.value, ',')[5].alias(\"CO2\").cast(DoubleType()),\n",
    "        split(df.value, ',')[6].alias(\"HumidityRatio\").cast(DoubleType()),\n",
    "        split(df.value, ',')[7].alias(\"Occupancy\").cast(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder realizar bien el ejercicio 2, convertimos la columna fecha a un formato timestamp para que la ventana la detecte, ya que sólo se realizan por columnas de event time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.withColumn(\"date\", regexp_replace(col(\"date\"), '\"', ''))\n",
    "df_data = df_data.withColumn('date2',to_timestamp(\"date\", \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calcular el promedio de valores de Temperatura, humedad relativa y concentración de CO2 para cada micro-batch y el promedio de dichos valores desde el arranque**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, realizamos la consulta con el modo de salida *update*, para cada micro-batch. Indicamos que los valores se vayan mostrando por la consola. Adicionalmente, especificamos que el intervalo de actualización sea de 5 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1_1 = (df_data.agg(avg(col(\"Temperature\")).alias('MB-AVG Temperature'),\n",
    "                          avg(col(\"Humidity\")).alias('MB-AVG Humidity'),\n",
    "                          avg(col(\"CO2\")).alias('MB-AVG CO2'))\n",
    "                         .writeStream\n",
    "                         .format('console')\n",
    "                         .trigger(processingTime= '5 seconds')\n",
    "                         .outputMode(\"update\")\n",
    "                         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_1_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-79b11b368aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_1_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result_1_1' is not defined"
     ]
    }
   ],
   "source": [
    "result_1_1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En segundo lugar, realizamos la consulta con el modo de salida *complete*, para cada los valores desde el arranque:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2\n",
    "result_1_2 = (df_data.agg(avg(col(\"Temperature\")).alias('AVG Temperature'),\n",
    "                        avg(col(\"Humidity\")).alias('AVG Humidity'),\n",
    "                        avg(col(\"CO2\")).alias('AVG CO2'))\n",
    "                        .writeStream\n",
    "                        .format('console')\n",
    "                        .trigger(processingTime= '5 seconds')\n",
    "                        .outputMode(\"complete\")\n",
    "                        .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_1_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5dbf64ae4cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_1_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result_1_2' is not defined"
     ]
    }
   ],
   "source": [
    "result_1_2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calcular el promedio de luminosidad en la estancia en ventanas deslizantes de tamaño 45 segundos, con un valor de deslizamiento de 15 segundos entre ventanas consecutivas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especificamos las características de las ventanas en el groupBy, éstas se definen a partir de la columna date (formato timestamp). Para cada ventana se calcula el promedio de 'Light':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = (df_data.groupBy(window(col(\"date\"), \"45 seconds\", \"15 seconds\"))\n",
    "                   .agg(avg('Light').alias('Light_avg'))\n",
    "                   .writeStream\\\n",
    "                   .format('console')\\\n",
    "                   .trigger(processingTime= '5 seconds')\\\n",
    "                   .outputMode(\"complete\")\\\n",
    "                   .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examinando los datos, podemos apreciar que el intervalo entre muestras originales no es exactamente de 1 minuto en muchos casos. Calcular el número de parejas de muestras consecutivas en cada micro-batch entre las cuales el intervalo de separación no es exactamente de 1 minuto.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como suele ser en pyspark\n",
    "#df_data = df_data.withColumn(\"date_lag\", lag(\"date\")).over(window.partitionBy('date').orderBy(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "df_data = df_data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Non-time-based windows are not supported on streaming DataFrames/Datasets;;\\nWindow [lag(date2#296, 1, null) windowspecdefinition(date2#296, date2#296 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag(date2, 1, NULL) OVER (PARTITION BY date2 ORDER BY date2 ASC NULLS FIRST unspecifiedframe$())#1153], [date2#296], [date2#296 ASC NULLS FIRST]\\n+- Aggregate [date2#296], [date2#296]\\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\\n                           +- Project [cast(value#8 as string) AS value#21]\\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1343.start.\n: org.apache.spark.sql.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;;\nWindow [lag(date2#296, 1, null) windowspecdefinition(date2#296, date2#296 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag(date2, 1, NULL) OVER (PARTITION BY date2 ORDER BY date2 ASC NULLS FIRST unspecifiedframe$())#1153], [date2#296], [date2#296 ASC NULLS FIRST]\n+- Aggregate [date2#296], [date2#296]\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp('date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp('date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\n                           +- Project [cast(value#8 as string) AS value#21]\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:389)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForStreaming$2.apply(UnsupportedOperationChecker.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForStreaming$2.apply(UnsupportedOperationChecker.scala:143)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:143)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:256)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)\n\tat sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-2521bf793877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'console'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'5 seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                    \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                    .start())\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Non-time-based windows are not supported on streaming DataFrames/Datasets;;\\nWindow [lag(date2#296, 1, null) windowspecdefinition(date2#296, date2#296 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag(date2, 1, NULL) OVER (PARTITION BY date2 ORDER BY date2 ASC NULLS FIRST unspecifiedframe$())#1153], [date2#296], [date2#296 ASC NULLS FIRST]\\n+- Aggregate [date2#296], [date2#296]\\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\\n                           +- Project [cast(value#8 as string) AS value#21]\\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n'"
     ]
    }
   ],
   "source": [
    "#prueba1\n",
    "diff_window = Window.partitionBy(\"date\").orderBy(\"date\")\n",
    "result_3 = (df_data.groupBy('date')\n",
    "                   .agg(lag('date').over(diff_window))\n",
    "                   .writeStream\\\n",
    "                   .format('console')\\\n",
    "                   .trigger(processingTime= '5 seconds')\\\n",
    "                   .outputMode(\"complete\")\\\n",
    "                   .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Non-time-based windows are not supported on streaming DataFrames/Datasets;;\\nWindow [lag(date#286, 1, null) windowspecdefinition(date#286, date#286 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_timestamp#1019], [date#286], [date#286 ASC NULLS FIRST]\\n+- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#296]\\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\\n                           +- Project [cast(value#8 as string) AS value#21]\\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1141.start.\n: org.apache.spark.sql.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;;\nWindow [lag(date#286, 1, null) windowspecdefinition(date#286, date#286 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_timestamp#1019], [date#286], [date#286 ASC NULLS FIRST]\n+- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#296]\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp('date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp('date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\n                           +- Project [cast(value#8 as string) AS value#21]\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:389)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForStreaming$2.apply(UnsupportedOperationChecker.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForStreaming$2.apply(UnsupportedOperationChecker.scala:143)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:143)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:256)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)\n\tat sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8ad47c1d9709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'console'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'5 seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                    \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Non-time-based windows are not supported on streaming DataFrames/Datasets;;\\nWindow [lag(date#286, 1, null) windowspecdefinition(date#286, date#286 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_timestamp#1019], [date#286], [date#286 ASC NULLS FIRST]\\n+- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#296]\\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\\n                           +- Project [cast(value#8 as string) AS value#21]\\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n'"
     ]
    }
   ],
   "source": [
    "#prueba2\n",
    "result_3 = df_data.withColumn(\"prev_timestamp\", lag(df_data.date).over(diff_window))\\\n",
    "                   .writeStream\\\n",
    "                   .format('console')\\\n",
    "                   .trigger(processingTime= '5 seconds')\\\n",
    "                   .outputMode(\"update\")\\\n",
    "                   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = (df_data.groupBy('row')\n",
    "                   .agg(first('date'),\n",
    "                       first('date2'))\n",
    "                   .writeStream\\\n",
    "                   .format('console')\\\n",
    "                   .outputMode(\"complete\")\\\n",
    "                   .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
