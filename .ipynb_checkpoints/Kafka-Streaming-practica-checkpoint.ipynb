{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  4. Ejemplo End-to-End Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentación Oficial Structured Spark Streaming**: http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**: https://blog.knoldus.com/basic-example-spark-structured-streaming-kafka-integration/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Instrucciones iniciales y setup\n",
    "\n",
    "2. Crea un directorio `checkpoint` dentro del subdirectorio `data`.\n",
    "\n",
    "3. Asegúrate de que tienes permisos suficientes para manipular archivos dentro del directorio (debería ser así ya, si has ejecutado los ejemplos previos). Si fuese necesario, ejecuta `sudo chmod -R 777 data`.\n",
    "\n",
    "**Entrada: cola de Kafka**\n",
    "\n",
    "4. Arranca el broker de Kafka, o bien localmente instalado o en una MV local o en un contenedor local (e.g. Docker).\n",
    "\n",
    "5. Modifica el script de Python `4-kafka_producer.py` para que envíe los datos al broker de Kafka (indicar la IP y puerto correctos).\n",
    "\n",
    "6. Activa si es necesario el entorno de Anaconda Python (**importante, usando Python v3.6+**). Ejecuta el productor de Kafka con `python p_kafka_producer.py 0.6 1.3 test data/occupancy_data.csv`.\n",
    "\n",
    "7. A partir de ese momento ya estás listo para ejecutar los *jobs* de Spark Streaming de este notebook. ¡Empecemos con el análisis!\n",
    "\n",
    "**WebUI**: Mientras el contexto de Spark Streaming esté activo, podemos acceder a la interfaz de monitorización de los *jobs* en http://localhost:4040."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Importaciones y creación del contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creación del SparkContext (solo la primera vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from operator import add\n",
    "from operator import sub\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_SUBMIT_ARGS =  --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 pyspark-shell \n",
      "\n",
      "JAVA_HOME =  /usr/lib/jvm/java-8-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "# Load external packages programatically\n",
    "import os\n",
    "# THIS IS MANDATORY\n",
    "# You must provide the information about the Maven artifact for the\n",
    "# Spark Streaming connector to Kafka\n",
    "# At present time, only the 0.8.2 version (deprecated) has\n",
    "# Python support\n",
    "#packages = \"org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5\"\n",
    "packages = \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages {0} pyspark-shell\".format(packages)\n",
    ")\n",
    "# THIS IS COMPULSORY\n",
    "# Comment the line below if JAVA_HOME is already set up or you\n",
    "# only have a single JVM version in your system\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "# OPTIONAL: Check setup of environment variables\n",
    "print(\"PYSPARK_SUBMIT_ARGS = \",os.environ[\"PYSPARK_SUBMIT_ARGS\"],\"\\n\")\n",
    "print(\"JAVA_HOME = \", os.environ[\"JAVA_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SparkContext(appName=\"KafkaStreamingEndtoEnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"prueba\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.159:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>prueba</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f256880dc10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del streaming context (en cada ejecución de ejercicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Métodos auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Método de parseo de datos meteorológicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método nos ayuda a parsear cada línea que llega por la cola de Kafka con datos meteorológicos. Lo utilizamos para acceder a los datos de cada evento (orden) del *stream* de entrada de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Métodos de escritura - Envío de datos a Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método contiene un *productor singleton* (para evitar tener más de un productor enviando datos al broker de Kafka) y un método para serializar los resultados en formato CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Fuente de datos - Lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "Schema = StructType([\n",
    " StructField(\"id\", IntegerType()),\n",
    " StructField(\"date\", StringType()),\n",
    " StructField(\"Temperature\", DoubleType()),\n",
    " StructField(\"Humidity\", DoubleType()),\n",
    " StructField(\"Light\", DoubleType()),\n",
    " StructField(\"CO2\", DoubleType()),\n",
    " StructField(\"HumidityRatio\", DoubleType()),\n",
    " StructField(\"Occupancy\", DoubleType())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrada de datos desde Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", 'localhost:9092')\\\n",
    "    .option('subscribe', 'test')\\\n",
    "    .load()\n",
    "#.schema(Schema) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr('CAST(value AS STRING)')\n",
    "\n",
    "df_data = df.select(\n",
    "        split(df.value, ',')[0].alias(\"row\").cast(StringType()),\n",
    "        split(df.value, ',')[1].alias(\"date\").cast(StringType()),\n",
    "        split(df.value, ',')[2].alias(\"Temperature\").cast(DoubleType()),\n",
    "        split(df.value, ',')[3].alias(\"Humidity\").cast(DoubleType()),\n",
    "        split(df.value, ',')[4].alias(\"Light\").cast(DoubleType()),\n",
    "        split(df.value, ',')[5].alias(\"CO2\").cast(DoubleType()),\n",
    "        split(df.value, ',')[6].alias(\"HumidityRatio\").cast(DoubleType()),\n",
    "        split(df.value, ',')[7].alias(\"Occupancy\").cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.withColumn(\"date\", regexp_replace(col(\"date\"), '\"', ''))\n",
    "df_data = df_data.withColumn('date2',to_timestamp(\"date\", \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular el promedio de valores de Temperatura, humedad relativa y concentración de CO2 para cada micro-batch y el promedio de dichos valores desde el arranque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve 'timewindow(date, 3000000, 300000000, 0)' due to data type mismatch: The slide duration (300000000) must be less than or equal to the windowDuration (3000000).;;\\n'Aggregate [timewindow(date#38, 3000000, 300000000, 0)], [timewindow(date#38, 3000000, 300000000, 0) AS window#47, avg(CO2#35) AS avg(CO2 AS `mean_CO2`)#58]\\n+- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as timestamp) AS date#38, cast(split(value#21, ,)[2] as double) AS Temperature#32, cast(split(value#21, ,)[3] as double) AS Humidity#33, cast(split(value#21, ,)[4] as int) AS Light#34, cast(split(value#21, ,)[5] as double) AS CO2#35, cast(split(value#21, ,)[6] as double) AS HumidityRatio#36, cast(split(value#21, ,)[7] as string) AS Occupancy#37]\\n   +- Project [cast(value#8 as string) AS value#21]\\n      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@ec9f01b, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@16b1f7be,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o125.agg.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'timewindow(date, 3000000, 300000000, 0)' due to data type mismatch: The slide duration (300000000) must be less than or equal to the windowDuration (3000000).;;\n'Aggregate [timewindow(date#38, 3000000, 300000000, 0)], [timewindow(date#38, 3000000, 300000000, 0) AS window#47, avg(CO2#35) AS avg(CO2 AS `mean_CO2`)#58]\n+- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as timestamp) AS date#38, cast(split(value#21, ,)[2] as double) AS Temperature#32, cast(split(value#21, ,)[3] as double) AS Humidity#33, cast(split(value#21, ,)[4] as int) AS Light#34, cast(split(value#21, ,)[5] as double) AS CO2#35, cast(split(value#21, ,)[6] as double) AS HumidityRatio#36, cast(split(value#21, ,)[7] as string) AS Occupancy#37]\n   +- Project [cast(value#8 as string) AS value#21]\n      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@ec9f01b, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@16b1f7be,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:224)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-18d30362a63c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresult_1_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3 seconds\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"5 minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     \u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CO2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean_CO2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'console'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             jdf = self._jgd.agg(exprs[0]._jc,\n\u001b[0;32m--> 115\u001b[0;31m                                 _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve 'timewindow(date, 3000000, 300000000, 0)' due to data type mismatch: The slide duration (300000000) must be less than or equal to the windowDuration (3000000).;;\\n'Aggregate [timewindow(date#38, 3000000, 300000000, 0)], [timewindow(date#38, 3000000, 300000000, 0) AS window#47, avg(CO2#35) AS avg(CO2 AS `mean_CO2`)#58]\\n+- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as timestamp) AS date#38, cast(split(value#21, ,)[2] as double) AS Temperature#32, cast(split(value#21, ,)[3] as double) AS Humidity#33, cast(split(value#21, ,)[4] as int) AS Light#34, cast(split(value#21, ,)[5] as double) AS CO2#35, cast(split(value#21, ,)[6] as double) AS HumidityRatio#36, cast(split(value#21, ,)[7] as string) AS Occupancy#37]\\n   +- Project [cast(value#8 as string) AS value#21]\\n      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@ec9f01b, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@16b1f7be,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n\""
     ]
    }
   ],
   "source": [
    "### from pyspark.sql.functions import col, avg \n",
    "\n",
    "# 1.1\n",
    "result_1_1 = df_data.groupBy(window(df_data.date, \"3 seconds\", \"5 minutes\"))\\\n",
    "                    .agg(avg(col(\"CO2\").alias('mean_CO2')))\\\n",
    "                    .writeStream\\\n",
    "                    .format('console')\\\n",
    "                    .trigger(processingTime= '5 seconds')\\\n",
    "                    .outputMode(\"append\")\\\n",
    "                    .start()\n",
    "\n",
    "#withWatermark(\"date\", \"3 seconds\")\\\n",
    "#.withWatermark(\"date\", \"3 seconds\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1_1 = (df_data.agg(avg(col(\"Temperature\")).alias('MB-AVG Temperature'),\n",
    "                          avg(col(\"Humidity\")).alias('MB-AVG Humidity'),\n",
    "                          avg(col(\"CO2\")).alias('MB-AVG CO2'))\n",
    "                         .writeStream\n",
    "                         .format('console')\n",
    "                         .trigger(processingTime= '5 seconds')\n",
    "                         .outputMode(\"update\")\n",
    "                         .start())\n",
    "\n",
    "#.withWatermark(\"date\", \"3 seconds\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_1_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-79b11b368aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_1_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result_1_1' is not defined"
     ]
    }
   ],
   "source": [
    "result_1_1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2\n",
    "result_1_2 = (df_data.agg(avg(col(\"Temperature\")).alias('AVG Temperature'),\n",
    "                        avg(col(\"Humidity\")).alias('AVG Humidity'),\n",
    "                        avg(col(\"CO2\")).alias('AVG CO2'))\n",
    "                        .writeStream\n",
    "                        .format('console')\n",
    "                        .trigger(processingTime= '5 seconds')\n",
    "                        .outputMode(\"complete\")\n",
    "                        .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_1_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5dbf64ae4cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_1_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result_1_2' is not defined"
     ]
    }
   ],
   "source": [
    "result_1_2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular el promedio de luminosidad en la estancia en ventanas deslizantes de tamaño 45 segundos, con un valor de deslizamiento de 15 segundos entre ventanas consecutivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = (df_data.groupBy(window(col(\"date\"), \"45 seconds\", \"15 seconds\"))\n",
    "                   .agg(avg('Light').alias('Light_avg'))\n",
    "                   .writeStream\\\n",
    "                   .format('console')\\\n",
    "                   .trigger(processingTime= '5 seconds')\\\n",
    "                   .outputMode(\"complete\")\\\n",
    "                   .start())\n",
    "#.avg(\"Light\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examinando los datos, podemos apreciar que el intervalo entre muestras originales no es exactamente de 1 minuto en muchos casos. Calcular el número de parejas de muestras consecutivas en cada micro-batch entre las cuales el intervalo de separación no es exactamente de 1 minuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como suele ser en pyspark\n",
    "#df_data = df_data.withColumn(\"date_lag\", lag(\"date\")).over(window.partitionBy('date').orderBy(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "df_data = df_data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Non-time-based windows are not supported on streaming DataFrames/Datasets;;\\nWindow [lag(date2#296, 1, null) windowspecdefinition(date2#296, date2#296 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag(date2, 1, NULL) OVER (PARTITION BY date2 ORDER BY date2 ASC NULLS FIRST unspecifiedframe$())#1153], [date2#296], [date2#296 ASC NULLS FIRST]\\n+- Aggregate [date2#296], [date2#296]\\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\\n                           +- Project [cast(value#8 as string) AS value#21]\\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1343.start.\n: org.apache.spark.sql.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;;\nWindow [lag(date2#296, 1, null) windowspecdefinition(date2#296, date2#296 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag(date2, 1, NULL) OVER (PARTITION BY date2 ORDER BY date2 ASC NULLS FIRST unspecifiedframe$())#1153], [date2#296], [date2#296 ASC NULLS FIRST]\n+- Aggregate [date2#296], [date2#296]\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp('date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp('date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\n                           +- Project [cast(value#8 as string) AS value#21]\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:389)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForStreaming$2.apply(UnsupportedOperationChecker.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForStreaming$2.apply(UnsupportedOperationChecker.scala:143)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:143)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:256)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)\n\tat sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-2521bf793877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'console'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'5 seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                    \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                    .start())\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Non-time-based windows are not supported on streaming DataFrames/Datasets;;\\nWindow [lag(date2#296, 1, null) windowspecdefinition(date2#296, date2#296 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS lag(date2, 1, NULL) OVER (PARTITION BY date2 ORDER BY date2 ASC NULLS FIRST unspecifiedframe$())#1153], [date2#296], [date2#296 ASC NULLS FIRST]\\n+- Aggregate [date2#296], [date2#296]\\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\\n                           +- Project [cast(value#8 as string) AS value#21]\\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n'"
     ]
    }
   ],
   "source": [
    "#prueba1\n",
    "diff_window = Window.partitionBy(\"date\").orderBy(\"date\")\n",
    "result_3 = (df_data.groupBy('date')\n",
    "                   .agg(lag('date').over(diff_window))\n",
    "                   .writeStream\\\n",
    "                   .format('console')\\\n",
    "                   .trigger(processingTime= '5 seconds')\\\n",
    "                   .outputMode(\"complete\")\\\n",
    "                   .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Non-time-based windows are not supported on streaming DataFrames/Datasets;;\\nWindow [lag(date#286, 1, null) windowspecdefinition(date#286, date#286 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_timestamp#1019], [date#286], [date#286 ASC NULLS FIRST]\\n+- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#296]\\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\\n                           +- Project [cast(value#8 as string) AS value#21]\\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1141.start.\n: org.apache.spark.sql.AnalysisException: Non-time-based windows are not supported on streaming DataFrames/Datasets;;\nWindow [lag(date#286, 1, null) windowspecdefinition(date#286, date#286 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_timestamp#1019], [date#286], [date#286 ASC NULLS FIRST]\n+- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#296]\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp('date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp('date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\n                           +- Project [cast(value#8 as string) AS value#21]\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:389)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForStreaming$2.apply(UnsupportedOperationChecker.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForStreaming$2.apply(UnsupportedOperationChecker.scala:143)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:143)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:256)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)\n\tat sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8ad47c1d9709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'console'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'5 seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                    \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Non-time-based windows are not supported on streaming DataFrames/Datasets;;\\nWindow [lag(date#286, 1, null) windowspecdefinition(date#286, date#286 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_timestamp#1019], [date#286], [date#286 ASC NULLS FIRST]\\n+- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#296]\\n   +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n      +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n         +- Filter AtLeastNNulls(n, row#31,date#286,Temperature#33,Humidity#34,Light#35,CO2#36,HumidityRatio#37,Occupancy#38,date2#296)\\n            +- Project [row#31, date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#296]\\n               +- Project [row#31, regexp_replace(date#47, \", ) AS date#286, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, date2#56]\\n                  +- Project [row#31, date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38, to_timestamp(\\'date, Some(yyyy-MM-dd HH:mm:ss)) AS date2#56]\\n                     +- Project [row#31, regexp_replace(date#32, \", ) AS date#47, Temperature#33, Humidity#34, Light#35, CO2#36, HumidityRatio#37, Occupancy#38]\\n                        +- Project [cast(split(value#21, ,)[0] as string) AS row#31, cast(split(value#21, ,)[1] as string) AS date#32, cast(split(value#21, ,)[2] as double) AS Temperature#33, cast(split(value#21, ,)[3] as double) AS Humidity#34, cast(split(value#21, ,)[4] as double) AS Light#35, cast(split(value#21, ,)[5] as double) AS CO2#36, cast(split(value#21, ,)[6] as double) AS HumidityRatio#37, cast(split(value#21, ,)[7] as string) AS Occupancy#38]\\n                           +- Project [cast(value#8 as string) AS value#21]\\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2952f3e5, kafka, Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3ab5a970,kafka,List(),None,List(),None,Map(sep -> ,, subscribe -> test, kafka.bootstrap.servers -> localhost:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n'"
     ]
    }
   ],
   "source": [
    "#prueba2\n",
    "result_3 = df_data.withColumn(\"prev_timestamp\", lag(df_data.date).over(diff_window))\\\n",
    "                   .writeStream\\\n",
    "                   .format('console')\\\n",
    "                   .trigger(processingTime= '5 seconds')\\\n",
    "                   .outputMode(\"update\")\\\n",
    "                   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = (df_data.groupBy('row')\n",
    "                   .agg(first('date'),\n",
    "                       first('date2'))\n",
    "                   .writeStream\\\n",
    "                   .format('console')\\\n",
    "                   .outputMode(\"complete\")\\\n",
    "                   .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
